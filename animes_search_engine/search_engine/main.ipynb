{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber nltk numpy rapidfuzz\n",
    "# !pip install cloudinary\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "import cloudinary\n",
    "from cloudinary import CloudinaryImage\n",
    "import cloudinary.uploader\n",
    "import cloudinary.api\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import regex as re\n",
    "import base64\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pickle\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz import process\n",
    "import json\n",
    "from custom_bm25 import BM25Okapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords_to_remove = (\n",
    "    'Score:', 'Episodes', 'Status', 'Aired', 'Premiered', 'Broadcast', 'Licensors',\n",
    "    'Source', 'Duration', 'Characters', 'Anime Details', 'Japanese', 'Main Supporting',\n",
    "    'Main Main', 'None', 'Demographic', 'Genres', 'Studios',\n",
    "    'Data source: MyAnimeList', 'Supporting Supporting', '[Written by MAL Rewrite]'\n",
    ")\n",
    "\n",
    "animes_corpus = []\n",
    "animes_json = []\n",
    "\n",
    "pdfs = glob.glob(r'C:\\Users\\menno\\Desktop\\AI-location\\animes_search_engine\\pdf_transformer\\pdfs\\*.pdf')\n",
    "\n",
    "for index, pdf in enumerate(pdfs):\n",
    "    with pdfplumber.open(pdf) as f:\n",
    "        \n",
    "        txt = ''.join(page.extract_text() for page in f.pages)\n",
    "        list_sentences = txt.split('\\n')\n",
    "        \n",
    "        # Filter sentences based on keywords\n",
    "        filtered_sentences = [\n",
    "            i for i in list_sentences if not any(i.startswith(keyword) for keyword in keywords_to_remove)\n",
    "        ]\n",
    "        animes_corpus.append('\\n'.join(filtered_sentences))\n",
    "\n",
    "        # Extract specific fields\n",
    "        def find_value(key, sentences):\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence.lower().startswith(key.lower()):\n",
    "                    return i\n",
    "            return None\n",
    "\n",
    "        score_index = find_value('score:', list_sentences)\n",
    "        genres_index = find_value('Genres', list_sentences)\n",
    "        description_end_index = find_value('Anime Details',list_sentences)\n",
    "        demographic_index = find_value('Demographic', list_sentences)\n",
    "        studios_index = find_value('Studios', list_sentences)\n",
    "        premiered_index = find_value('Premiered', list_sentences)\n",
    "\n",
    "        # Handling fields\n",
    "        def extract_field_value(index, label):\n",
    "            if index is not None:\n",
    "                parts = list_sentences[index].split(label)\n",
    "                return parts[1].strip() if len(parts) > 1 else list_sentences[index + 1].strip()\n",
    "            return None\n",
    "\n",
    "        demographic_value = extract_field_value(demographic_index, 'Demographic')\n",
    "        studios_value = extract_field_value(studios_index, 'Studios')\n",
    "        premiered_value = extract_field_value(premiered_index, 'Premiered') \n",
    "        genres_value =  list_sentences[genres_index].split('Genres')[1].strip().split( 'Demographic')[0].split('Duration')[0].split(', ') if genres_index else None\n",
    "\n",
    "        premiered_list = premiered_value.split() if premiered_value is not None else None \n",
    "\n",
    "        # JSON format output per doc\n",
    "        animes_json.append({\n",
    "            'doc_name': os.path.basename(pdf),\n",
    "            'title': f.metadata.get('Title'),\n",
    "            'image':base64.b64encode(f.images[0][\"stream\"].get_data()).decode('utf-8')  ,\n",
    "            'score': list_sentences[score_index].split()[1],\n",
    "            'description': '\\n'.join(list_sentences[score_index + 1:description_end_index]) if score_index is not None and description_end_index is not None else None,\n",
    "            'genres': genres_value,\n",
    "            'demographic': demographic_value.split('Duration')[0] if demographic_value is not None else None,\n",
    "            'studios': studios_value,\n",
    "            'premiered': {\n",
    "                \"season\":premiered_list[0]  ,\n",
    "                \"year\": premiered_list[1]\n",
    "            } if premiered_list is not None else None\n",
    "        })\n",
    "        \n",
    "    print(f'anime {index + 1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(animes_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cloudinary.Config at 0x24b41704710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() \n",
    "\n",
    "cloudinary.config( \n",
    "  cloud_name = os.getenv(\"CLOUD_NAME\"), \n",
    "  api_key = os.getenv(\"API_KEY\"), \n",
    "  api_secret = os.getenv(\"API_SECRET\"),\n",
    "  secure = True\n",
    ")\n",
    "\n",
    "\n",
    "def upload_base64_to_cloudinary(base64_image, folder=\"animes\"):\n",
    "    \n",
    "    try:\n",
    "        # Upload to Cloudinary with folder specification\n",
    "        response = cloudinary.uploader.upload(\n",
    "            f\"data:image/png;base64,{base64_image}\",\n",
    "            folder=folder,  # Specify the folder\n",
    "            resource_type=\"auto\"  # Automatically detect resource type\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading image: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your anime list\n",
    "i = 1\n",
    "for anime in animes_json:\n",
    "    try:\n",
    "        # Upload the image and get response\n",
    "        response = upload_base64_to_cloudinary(anime['image'])\n",
    "        \n",
    "        if response:\n",
    "            i = i+1\n",
    "\n",
    "            print(f\"Successfully uploaded image for {anime['doc_name']} anime number {i} \")\n",
    "            # Optionally store the URL back in your anime dictionary\n",
    "            anime['image'] = response['secure_url']\n",
    "        else:\n",
    "            print(f\"Failed to upload image for {anime['doc_name']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing anime: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save animes_json to a file\n",
    "with open('animes_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(animes_json, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"animes_json has been saved to 'animes_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower, remove punctuations , remove stop words , tokenize input\n",
    "def filter_text(input_string , string , stop_words):\n",
    "    tokenized_input = input_string.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "    filterd_tokenized_input = [w for w in tokenized_input if not w in stop_words]\n",
    "    filterd_tokenized_input = [ps.stem(lemmatizer.lemmatize(i)) for i in filterd_tokenized_input ]\n",
    "    return filterd_tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_cleaned_corpus = []\n",
    "for doc in animes_corpus:\n",
    "    clean_doc = filter_text(doc , string , stop_words)\n",
    "    tokenized_cleaned_corpus.append(clean_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_remove_duplicates(nested_list):\n",
    "    # Flatten the list using recursion\n",
    "    def flatten(lst):\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                yield from flatten(item)\n",
    "            else:\n",
    "                yield item\n",
    "\n",
    "    # Flatten the list and convert it to a set to remove duplicates\n",
    "    flattened = list(flatten(nested_list))\n",
    "    unique_items = []\n",
    "    for item in flattened:\n",
    "        if len(item) > 2 and item not in unique_items:  # Skip items with length 1 and 2\n",
    "            unique_items.append(item)\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "# flatten_corpus = flatten_and_remove_duplicates(tokenized_cleaned_corpus)\n",
    "\n",
    "list_to_flatten = []\n",
    "\n",
    "for doc in animes_corpus:\n",
    "    clean_doc = doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "    list_to_flatten.append(clean_doc)\n",
    "\n",
    "flatten_corpus = flatten_and_remove_duplicates(list_to_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(flatten_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flatten_corpus.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(flatten_corpus, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save model\n",
    "file_name = r\"models\\model.pkl\"\n",
    "\n",
    "os.makedirs(os.path.dirname(file_name),exist_ok=True)\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(bm25, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the bm25 model\n",
    "with open(r\"models\\model.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# loading the flatten corpus for the fuzzy search\n",
    "with open('flatten_corpus.json', 'r', encoding='utf-8') as json_file:\n",
    "    flatten_corpus = json.load(json_file)\n",
    "\n",
    "# loading the the pdfs jsons : results\n",
    "with open('animes_data.json', 'r', encoding='utf-8') as json_file:\n",
    "    animes_json = json.load(json_file)\n",
    "\n",
    "doc_names = [anime['title'] for anime in animes_json]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"one piece\"\n",
    "\n",
    "tokenized_query = query.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "\n",
    "fuzzy_tokenized_query_list = []\n",
    "\n",
    "for q in tokenized_query:\n",
    "    if len(q) <= 2:\n",
    "        q=q\n",
    "    else :\n",
    "        # print(process.extract(q,flatten_corpus,limit=3))\n",
    "        fuzzy_query = process.extractOne(q, flatten_corpus)\n",
    "        q = fuzzy_query[0] if fuzzy_query[1] > 79 else q\n",
    "        \n",
    "    fuzzy_tokenized_query_list.append(q)\n",
    "\n",
    "fuzzy_tokenized_query = ' '.join(fuzzy_tokenized_query_list)\n",
    "print(fuzzy_tokenized_query)\n",
    "\n",
    "fuzzy_tokenized_cleaned_query = filter_text(fuzzy_tokenized_query , string , stop_words)\n",
    "print(fuzzy_tokenized_cleaned_query)\n",
    "\n",
    "result = model.get_top_n(fuzzy_tokenized_cleaned_query , doc_names, n = 100)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# query = \"one one one one one\"\n",
    "# tokenized_cleaned_query = filter_text(query , string , stop_words)\n",
    "\n",
    "# fuzzy_tokenized_cleaned_query = []\n",
    "\n",
    "# for q in tokenized_cleaned_query:\n",
    "#     if len(q) <= 2:\n",
    "#         q=q\n",
    "#     else :\n",
    "#         q = process.extractOne(q, flatten_corpus)[0]\n",
    "#         print(process.extract(q,flatten_corpus,limit=2))\n",
    "#     fuzzy_tokenized_cleaned_query.append(q)\n",
    "\n",
    "# print(fuzzy_tokenized_cleaned_query)\n",
    "\n",
    "\n",
    "# result = model.get_top_n(fuzzy_tokenized_cleaned_query , doc_names, n = len(doc_names))\n",
    "# print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
